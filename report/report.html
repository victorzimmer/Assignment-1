<!DOCTYPE html>
<html>
<head>
<title>report.md</title>
<meta http-equiv="Content-type" content="text/html;charset=UTF-8">

<style>
/* https://github.com/microsoft/vscode/blob/master/extensions/markdown-language-features/media/markdown.css */
/*---------------------------------------------------------------------------------------------
 *  Copyright (c) Microsoft Corporation. All rights reserved.
 *  Licensed under the MIT License. See License.txt in the project root for license information.
 *--------------------------------------------------------------------------------------------*/

body {
	font-family: var(--vscode-markdown-font-family, -apple-system, BlinkMacSystemFont, "Segoe WPC", "Segoe UI", "Ubuntu", "Droid Sans", sans-serif);
	font-size: var(--vscode-markdown-font-size, 14px);
	padding: 0 26px;
	line-height: var(--vscode-markdown-line-height, 22px);
	word-wrap: break-word;
}

#code-csp-warning {
	position: fixed;
	top: 0;
	right: 0;
	color: white;
	margin: 16px;
	text-align: center;
	font-size: 12px;
	font-family: sans-serif;
	background-color:#444444;
	cursor: pointer;
	padding: 6px;
	box-shadow: 1px 1px 1px rgba(0,0,0,.25);
}

#code-csp-warning:hover {
	text-decoration: none;
	background-color:#007acc;
	box-shadow: 2px 2px 2px rgba(0,0,0,.25);
}

body.scrollBeyondLastLine {
	margin-bottom: calc(100vh - 22px);
}

body.showEditorSelection .code-line {
	position: relative;
}

body.showEditorSelection .code-active-line:before,
body.showEditorSelection .code-line:hover:before {
	content: "";
	display: block;
	position: absolute;
	top: 0;
	left: -12px;
	height: 100%;
}

body.showEditorSelection li.code-active-line:before,
body.showEditorSelection li.code-line:hover:before {
	left: -30px;
}

.vscode-light.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(0, 0, 0, 0.15);
}

.vscode-light.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(0, 0, 0, 0.40);
}

.vscode-light.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

.vscode-dark.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(255, 255, 255, 0.4);
}

.vscode-dark.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(255, 255, 255, 0.60);
}

.vscode-dark.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

.vscode-high-contrast.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(255, 160, 0, 0.7);
}

.vscode-high-contrast.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(255, 160, 0, 1);
}

.vscode-high-contrast.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

img {
	max-width: 100%;
	max-height: 100%;
}

a {
	text-decoration: none;
}

a:hover {
	text-decoration: underline;
}

a:focus,
input:focus,
select:focus,
textarea:focus {
	outline: 1px solid -webkit-focus-ring-color;
	outline-offset: -1px;
}

hr {
	border: 0;
	height: 2px;
	border-bottom: 2px solid;
}

h1 {
	padding-bottom: 0.3em;
	line-height: 1.2;
	border-bottom-width: 1px;
	border-bottom-style: solid;
}

h1, h2, h3 {
	font-weight: normal;
}

table {
	border-collapse: collapse;
}

table > thead > tr > th {
	text-align: left;
	border-bottom: 1px solid;
}

table > thead > tr > th,
table > thead > tr > td,
table > tbody > tr > th,
table > tbody > tr > td {
	padding: 5px 10px;
}

table > tbody > tr + tr > td {
	border-top: 1px solid;
}

blockquote {
	margin: 0 7px 0 5px;
	padding: 0 16px 0 10px;
	border-left-width: 5px;
	border-left-style: solid;
}

code {
	font-family: Menlo, Monaco, Consolas, "Droid Sans Mono", "Courier New", monospace, "Droid Sans Fallback";
	font-size: 1em;
	line-height: 1.357em;
}

body.wordWrap pre {
	white-space: pre-wrap;
}

pre:not(.hljs),
pre.hljs code > div {
	padding: 16px;
	border-radius: 3px;
	overflow: auto;
}

pre code {
	color: var(--vscode-editor-foreground);
	tab-size: 4;
}

/** Theming */

.vscode-light pre {
	background-color: rgba(220, 220, 220, 0.4);
}

.vscode-dark pre {
	background-color: rgba(10, 10, 10, 0.4);
}

.vscode-high-contrast pre {
	background-color: rgb(0, 0, 0);
}

.vscode-high-contrast h1 {
	border-color: rgb(0, 0, 0);
}

.vscode-light table > thead > tr > th {
	border-color: rgba(0, 0, 0, 0.69);
}

.vscode-dark table > thead > tr > th {
	border-color: rgba(255, 255, 255, 0.69);
}

.vscode-light h1,
.vscode-light hr,
.vscode-light table > tbody > tr + tr > td {
	border-color: rgba(0, 0, 0, 0.18);
}

.vscode-dark h1,
.vscode-dark hr,
.vscode-dark table > tbody > tr + tr > td {
	border-color: rgba(255, 255, 255, 0.18);
}

</style>

<style>
/* Tomorrow Theme */
/* http://jmblog.github.com/color-themes-for-google-code-highlightjs */
/* Original theme - https://github.com/chriskempson/tomorrow-theme */

/* Tomorrow Comment */
.hljs-comment,
.hljs-quote {
	color: #8e908c;
}

/* Tomorrow Red */
.hljs-variable,
.hljs-template-variable,
.hljs-tag,
.hljs-name,
.hljs-selector-id,
.hljs-selector-class,
.hljs-regexp,
.hljs-deletion {
	color: #c82829;
}

/* Tomorrow Orange */
.hljs-number,
.hljs-built_in,
.hljs-builtin-name,
.hljs-literal,
.hljs-type,
.hljs-params,
.hljs-meta,
.hljs-link {
	color: #f5871f;
}

/* Tomorrow Yellow */
.hljs-attribute {
	color: #eab700;
}

/* Tomorrow Green */
.hljs-string,
.hljs-symbol,
.hljs-bullet,
.hljs-addition {
	color: #718c00;
}

/* Tomorrow Blue */
.hljs-title,
.hljs-section {
	color: #4271ae;
}

/* Tomorrow Purple */
.hljs-keyword,
.hljs-selector-tag {
	color: #8959a8;
}

.hljs {
	display: block;
	overflow-x: auto;
	color: #4d4d4c;
	padding: 0.5em;
}

.hljs-emphasis {
	font-style: italic;
}

.hljs-strong {
	font-weight: bold;
}
</style>

<style>
/*
 * Markdown PDF CSS
 */

 body {
	font-family: -apple-system, BlinkMacSystemFont, "Segoe WPC", "Segoe UI", "Ubuntu", "Droid Sans", sans-serif, "Meiryo";
	padding: 0 12px;
}

pre {
	background-color: #f8f8f8;
	border: 1px solid #cccccc;
	border-radius: 3px;
	overflow-x: auto;
	white-space: pre-wrap;
	overflow-wrap: break-word;
}

pre:not(.hljs) {
	padding: 23px;
	line-height: 19px;
}

blockquote {
	background: rgba(127, 127, 127, 0.1);
	border-color: rgba(0, 122, 204, 0.5);
}

.emoji {
	height: 1.4em;
}

code {
	font-size: 14px;
	line-height: 19px;
}

/* for inline code */
:not(pre):not(.hljs) > code {
	color: #C9AE75; /* Change the old color so it seems less like an error */
	font-size: inherit;
}

/* Page Break : use <div class="page"/> to insert page break
-------------------------------------------------------- */
.page {
	page-break-after: always;
}

</style>

<script src="https://unpkg.com/mermaid/dist/mermaid.min.js"></script>
</head>
<body>
  <script>
    mermaid.initialize({
      startOnLoad: true,
      theme: document.body.classList.contains('vscode-dark') || document.body.classList.contains('vscode-high-contrast')
          ? 'dark'
          : 'default'
    });
  </script>
<h1 id="assignment-1">Assignment 1</h1>
<h2 id="fys-2021-machine-learning-h22">FYS-2021 Machine Learning H22</h2>
<!-- ### Victor Zimmer, victor.zimmer@uit.no, vzi002 -->
<br />
<h2 id="problem-1">Problem 1</h2>
<p>With the standard model $r = f(x) + \epsilon$</p>
<p>Multivariate linear regression starts from a very simple basis with the equation for any given line $y = mx + n$ where $y$ is a dependent variable of the independent variable $x$ and $mx + n$ defines a line in 2D space. For this line $n$ is the intersect with the y-axis, that is the value of $y$ at $x=0$, and $mx$ represents the rate $m$ in relation to the change in $x$.</p>
<p>From this simple basis univariate linear regression is easily derived to find a line that best fits the trend in a given dataset. Often its represented as $\hat{y} = \theta_0 + \theta_1 x$ where $\hat{y}$ is the response or estimate, based on variables in the dataset. $\theta$ is used for the tunable values of the equation with $\theta_0$ being the equivalent of $n$ in our previous line equation and $\theta_1$ corresponding to $m$.</p>
<p>Univariate linear regression can relative easily be tuned to fit the trend of a given dataset, for example using the least squares method.</p>
<p>In the model $r = f(x) + \epsilon,$ this is extended to multivariate linear regression. Our estimate function $g(x|\theta)$ gives us $g(x) = \theta_0 + \theta_1 x_1 + \theta_2 x_2 + ... + \theta_K x_K$ where $K$ is the length of our variable set. \theta_0 gets a special name with it being our <em>bias</em>. \epsilon represents external variables that are not caught by our model.</p>
<p>We begin by initializing \theta with random values, which gives a poor regression, and then tuning that such that the regression improves. With enough steps this should give a better, and maybe even satisfactory, regression.</p>
<p>To be able to tune \theta we need a measure of the error from the desired output, for the given dataset, this is done with a <em>cost function</em>. This looks a lot like a multivariate extension of the least squares method, with the cost function defined as $C(\theta) = \frac{1}{2K}\sum{(\hat{y}_k - y_k)^2}$.</p>
<p>The cost function simply subtracts the true output from the estimated output and squares it removing negativity, this is averaged for the dataset to give a measure of the error for a given input and output.</p>
<p>This can be used to update our theta values to improve our regression for a given input and output. Pairing this with a scaled learning rate we can train the regression on a dataset by iteratively running steps of tuning for all variables in our training data set. This learning can be done by slightly adjusting our values for $\theta$ in the direction that reduces the difference between our estimation and our expected output, which is equivalent to adjusting the values of $\theta$ slightly based on the derivative of the cost function, in the direction that reduces our cost. For any value of $\theta$ this looks like $\theta_0 = \theta_0 - \alpha \frac{d}{d \theta_0}C(\theta_0)$, where $\alpha$ is a constant learning step size to scale our change per step.</p>
<p>This requires a differentiation of our cost function with respect to a \theta_k $C(\theta) = \frac{1}{2K}\sum{(\hat{y}_k - y_k)^2}$ which gives us $\frac{d}{d \theta_k}C(\theta) = \frac{1}{K}\sum{(\hat{y}_k - y_k)x_k}$. We are not asked to prove this partial differentiation, but if we wanted to we could do so simply by using the chain rule and treating $(\hat{y}_k - y_k)$ as an inner function.</p>
<h4 id="implementation">Implementation</h4>
<p>I implemented this in three phases.</p>
<p><strong>First phase</strong> was the implementation of the above math as code in the functions <code>rs, cost, tuneTheta, predictDataset, learnDataset</code></p>
<p><code>rs(theta, x)</code> simply uses the given theta values and x values to make predictions for a y-value, this is done by multiplying together the theta and x-values and summing them. It is this function $g(x|\theta)$ gives us $g(x) = \theta_0 + \theta_1 x_1 + \theta_2 x_2 + ... + \theta_K x_K$ with an $x_0$ implemented as $x_0 = 1$ for convinience.</p>
<p><code>cost(theta, x, y)</code> calculates the cost of a given set of theta values. It is this function $C(\theta) = \frac{1}{2K}\sum{(\hat{y}_k - y_k)^2}$.</p>
<p><code>tuneTheta(theta, x, y, learningRate, iterations)</code> runs a given amount of iterations attempting to nudge the values for theta according to $\theta_k = \theta_k - \alpha \frac{d}{d \theta_k}C(\theta_k) = \frac{1}{K}\sum{(\hat{y}_k - y_k)x_k}$. The value for $\alpha$ is given by the learning rate.</p>
<p><code>predictDataset(dataset, theta)</code> is implemented as a utility for further code, but it returns a prediction for a y, using <code>rs()</code>, given theta values and a dataset. The goal is for it to accept a dataset similar to <code>learnDataset</code>.</p>
<p><code>learnDataset(dataset, learningRate, iterations)</code> takes in an entire dataset and uses the provided learning rate and iteration count to run the functions above to optimize theta values. It initializes theta as random numbers between zero and one, any number could be used, but using random numbers allow for rerunning the algorithm to potentially result in a better (or worse) model. It then returns the optimized theta values.</p>
<p><strong>Second phase</strong> was the implementation of the leave-one-out cross-validation which is implemented in the <code>loocv(learningRate, iterations)</code> function. It allows for setting the number of iterations and learning rate, and then it uses the read dataset. It iteratively splits the dataset into a validation set containing one row from the original dataset and a training set containing the rest, then it train a model using the training set before using this model to predict the value in the validation set (the one left out). It returns a dictionary containing the data from its run.</p>
<p><strong>Third phase</strong> was the implementation of plotting and statistical numbers for the model. The plotting is done in <code>plotData()</code> which uses <code>loocv()</code> to run the model(s), and then uses <em>matplotlib</em> to plot the data with a scatter plot for both the predicted and true data, a histogram for the errors, and a title with the RMSE and R^2</p>
<p><em>Figure 1</em></p>
<p><img src="./Figure_1.png" alt="Figure 1"></p>
<p>After implementing the third phase I decided to normalize the x-values in the dataset as the predictions was quite bad. Normalization helped, probably because it equalizes the effect of all $x_i$, but still the model(s) are far from perfect. In figure 1 we can see the result of a run before normalization, with $R^2 &lt;&lt; 0$ telling us that its far worse than a simply the mean of the data. In figure 2, which is with normalization, this has improved considerably, however we still have $R^2 &lt; 0$ telling us the model is slightly worse than the mean.</p>
<p><em>Figure 2</em></p>
<p><img src="./Figure_2.png" alt="Figure 2"></p>
<!-- # Other stuff




$\hat{y} = \beta_0 + \beta_1x_1 + \epsilon$

Assumptions:
x independent, y dependent on x (fit for regression analysis)
linear correlation
continoous

r = response

f(x) = true function

$g(x|\theta)$ = estimate function

x is a variable in a univariate model and a vector in a multivariate model

$\theta$ variables for function $r(x) = \theta_0 + \theta_1 x$

$\epsilon$ error

Least squares method
Regression line


### Task 1b

The learnable parameters are $\theta$, they define the slope and intersect of the regression line. -->
<h2 id="problem-2">Problem 2</h2>
<h3 id="task-a">Task a</h3>
<p>$\hat{\mu}$</p>
<p>Likelihood function $L(\mu, x) = \prod_{t=1}^{n_1}{ \frac{1}{\sqrt{2\pi}\sigma} e^{-\frac{1}{2} (\frac{x_1^t-\mu}{\sigma})^2 } }$</p>
<p>Maximum likelihood $\hat{\mu} = max L()$</p>
<p>An optimization for the likelihood function will also optimize the log-likelihood.</p>
<p>The log likelihood converts the product to a sum $l(\mu, x) =\sum_{t=1}^{n_1}{[-\ln{\sqrt{2\pi}\sigma} - \frac{1}{2}(\frac{x_1^t-\mu}{\sigma})^2]} = - n_1 \ln{\sqrt{2\pi}\sigma} - \frac{1}{2} \sum_{t=1}^{n_1}{(\frac{x_1^t-\mu}{\sigma})^2}$</p>
<p>Differentiation with respect to $\mu$ gives $\frac{\delta}{\delta\mu} l(\mu, x) = \frac{1}{\sigma}\sum_{t=1}^{n_1}{(\frac{x_1^t-\mu}{\sigma})}$</p>
<p>We set this to zero $\frac{1}{\sigma}\sum_{t=1}^{n_1}{(\frac{x_1^t-\mu}{\sigma})} = 0$ giving us $\hat{\mu} = \frac{1}{n_1} \sum_{t=1}^{n_1}{x_1^t}$</p>

</body>
</html>
